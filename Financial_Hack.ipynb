{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install -q -U transformers accelerate\n",
        "!pip install -q sentence-transformers faiss-cpu pandas tqdm\n",
        "import pandas as pd\n",
        "import torch\n",
        "import re\n",
        "import faiss\n",
        "import numpy as np\n",
        "from tqdm.auto import tqdm\n",
        "import huggingface_hub\n",
        "from typing import Optional, List, Dict\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "class QuantitativeStockAnalyzer:\n",
        "    def analyze(self, context: str, tweets_str: str) -> (str, str):\n",
        "        try:\n",
        "            lines = context.strip().split('\\n')\n",
        "            if 'date,open,high,low,close' in lines[0].lower(): df = pd.read_csv(pd.io.common.StringIO('\\n'.join(lines)))\n",
        "            else:\n",
        "                headers = ['date','open','high','low','close','adj-close','inc-5','inc-10','inc-15','inc-20','inc-25','inc-30']\n",
        "                df = pd.read_csv(pd.io.common.StringIO('\\n'.join(lines)), header=None, names=headers[:len(lines[0].split(','))])\n",
        "            df['close'] = pd.to_numeric(df['close'], errors='coerce').dropna()\n",
        "            if len(df['close']) >= 10:\n",
        "                ma5 = df['close'].rolling(window=5).mean().iloc[-1]\n",
        "                ma10 = df['close'].rolling(window=10).mean().iloc[-1]\n",
        "                tech_summary = \"Bullish short-term trend (5-day MA > 10-day MA).\" if ma5 > ma10 else \"Bearish short-term trend (5-day MA < 10-day MA).\"\n",
        "            else: tech_summary = \"Insufficient data for moving average.\"\n",
        "        except: tech_summary = \"Technical data parsing failed.\"\n",
        "        positive_keywords = ['bullish', 'gains', 'upgraded', 'highs', 'beat', 'positive', 'good', 'soaring', 'growth', 'rebound', 'buying', 'breakout', 'support', 'boosted', 'reiterates']\n",
        "        negative_keywords = ['bearish', 'loss', 'downgraded', 'lows', 'miss', 'negative', 'flaw', 'risk', 'sells', 'cuts', 'weakness', 'decline', 'drop', 'warning', 'pressure']\n",
        "        pos_count = sum(1 for word in positive_keywords if word in tweets_str.lower())\n",
        "        neg_count = sum(1 for word in negative_keywords if word in tweets_str.lower())\n",
        "        net_sentiment = pos_count - neg_count\n",
        "        if net_sentiment > 1: sentiment_summary = \"Strongly Positive\"\n",
        "        elif net_sentiment > 0: sentiment_summary = \"Slightly Positive\"\n",
        "        elif net_sentiment < -1: sentiment_summary = \"Strongly Negative\"\n",
        "        elif net_sentiment < 0: sentiment_summary = \"Slightly Negative\"\n",
        "        else: sentiment_summary = \"Neutral\"\n",
        "        return tech_summary, f\"Net sentiment from tweets is {sentiment_summary}.\"\n",
        "\n",
        "class ComplianceAgentRAG:\n",
        "    def __init__(self):\n",
        "        self.principles = [\"Principle: Fiduciary Duty. Action: An advisor must prioritize the client's interests. Breach: Recommending a high-commission fund unsuitable for a client's risk profile.\",\"Principle: Suitability. Action: Recommendations must match the client's risk tolerance, goals, and situation.\",\"Principle: Full Disclosure. Action: All risks, fees, and conflicts of interest must be clearly explained.\",\"Principle: AML/CFT. Action: Report suspicious transactions (e.g., from high-risk jurisdictions, unclear source of funds) to AMLO and refuse onboarding if CDD fails.\",\"Principle: Data Privacy (PDPA). Action: Must obtain explicit, informed consent before sharing client data with third parties.\",\"Principle: Market Integrity. Action: Prohibit creating artificial volume or price movement.\",\"Principle: Cybersecurity. Action: Critical vulnerabilities must be patched immediately. Incidents must be reported to the BOT.\"]\n",
        "        self.sbert_model = SentenceTransformer('all-MiniLM-L6-v2', device='cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        self.index = self._build_index()\n",
        "    def _build_index(self):\n",
        "        embeddings = self.sbert_model.encode(self.principles, convert_to_tensor=True)\n",
        "        index = faiss.IndexFlatL2(embeddings.shape[1]); index.add(embeddings.cpu().detach().numpy())\n",
        "        return index\n",
        "    def retrieve_relevant_principles(self, query: str, k: int = 3) -> List[str]:\n",
        "        query_embedding = self.sbert_model.encode([query]); _, indices = self.index.search(query_embedding, k)\n",
        "        return [self.principles[i] for i in indices[0]]\n",
        "\n",
        "def classify_query(query: str) -> str:\n",
        "    q_lower = query.lower()\n",
        "    if 'project whether the closing price' in q_lower or 'ราคาปิดของ' in q_lower: return \"stock_prediction\"\n",
        "    if 'most appropriate action' in q_lower or 'แนวทางที่เหมาะสมที่สุด' in q_lower or 'best aligns with the ethical' in q_lower: return \"ethical_scenario\"\n",
        "    return \"general_mcq\"\n",
        "\n",
        "def get_surgical_prompt(prompt_type: str, **kwargs) -> Dict[str, str]:\n",
        "    system_prompt = \"You are Typhoon, an elite financial analyst with a CFA charter, renowned for your accuracy and ethical judgment. You will reason step-by-step before providing a final, concise answer.\"\n",
        "    # --- REINFORCED PROMPT FOR STOCK PREDICTION ---\n",
        "    if prompt_type == \"stock_prediction\":\n",
        "        user_prompt = f\"\"\"**Task:** Predict the stock price movement.\\n\\n**Chain of Thought:**\\n1. **Technical Analysis:** The trend is: {kwargs['tech_analysis']}.\\n2. **Sentiment Analysis:** The market mood is: {kwargs['sentiment_analysis']}.\\n3. **Synthesis & Conclusion:** Weigh the technicals against the sentiment to make a definitive prediction.\\n\\n**Full Context:**\\n{kwargs['query']}\\n\\nAfter your reasoning, you MUST conclude with a single line containing ONLY the final answer: \"Final Answer: [Rise/Fall]\".\"\"\"\n",
        "    elif prompt_type == \"ethical_scenario\":\n",
        "        principles_str = \"\\n- \".join(kwargs['principles'])\n",
        "        user_prompt = f\"\"\"**Task:** Identify the MOST appropriate action based on Thai SEC/BOT regulations.\\n\\n**Chain of Thought:**\\n1. **Core Issue:** The central conflict here is about {kwargs.get('issue', 'regulatory compliance')}.\\n2. **Guiding Principles:** The most relevant principles are:\\n- {principles_str}\\n3. **Option Evaluation:** Assess each option (A, B, C, D) against these principles.\\n4. **Conclusion:** Select the single best option.\\n\\n**Scenario:**\\n{kwargs['query']}\\n\\nAfter your reasoning, conclude with the line \"Final Answer: [letter]\".\"\"\"\n",
        "    else: # general_mcq\n",
        "        user_prompt = f\"\"\"**Task:** Answer the following multiple-choice question correctly.\\n\\n**Chain of Thought:**\\n1. **Question Analysis:** The question tests the concept of {kwargs.get('concept', 'a financial principle')}.\\n2. **Option Evaluation:** Assess options A, B, C, D, and E.\\n3. **Conclusion:** State the correct option.\\n\\n**Question:**\\n{kwargs['query']}\\n\\nAfter your reasoning, conclude with the line \"Final Answer: [letter]\".\"\"\"\n",
        "    return {\"system\": system_prompt, \"user\": user_prompt}\n",
        "\n",
        "MODEL_ID = \"KBTG-Labs/THaLLE-0.1-7B-fa\"\n",
        "\n",
        "def inference(messages: List[Dict[str, str]], model, tokenizer) -> str:\n",
        "    prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "    model_inputs = tokenizer([prompt], return_tensors=\"pt\").to(model.device)\n",
        "    generated_ids = model.generate(model_inputs.input_ids, max_new_tokens=350, do_sample=False)\n",
        "    generated_ids = [output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)]\n",
        "    response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
        "    return response\n",
        "\n",
        "def extract_answer(response_text: str, query_type: str) -> Optional[str]:\n",
        "    \"\"\"\n",
        "    Extracts the final answer from the model's response with prioritization.\n",
        "    \"\"\"\n",
        "    # Priority 1: Look for the explicit \"Final Answer:\" pattern from our CoT prompt.\n",
        "    final_answer_match = re.search(r\"Final Answer:\\s*(Rise|Fall|[A-E])\", response_text, re.IGNORECASE)\n",
        "    if final_answer_match:\n",
        "        ans = final_answer_match.group(1).capitalize()\n",
        "        # Capitalize() makes 'rise' -> 'Rise', 'a' -> 'A'\n",
        "        if ans in [\"Rise\", \"Fall\"]:\n",
        "            return ans\n",
        "        return ans.upper()\n",
        "\n",
        "    # Priority 2 (for stock predictions): If CoT fails, search the whole text for the keywords.\n",
        "    # This is a strong fallback for Rise/Fall questions.\n",
        "    if query_type == \"stock_prediction\":\n",
        "        if 'rise' in response_text.lower() or 'ขึ้น' in response_text: return 'Rise'\n",
        "        if 'fall' in response_text.lower() or 'ลง' in response_text: return 'Fall'\n",
        "\n",
        "    # Priority 3 (for MCQs): Find the last mentioned capital letter A-E. This often is the model's conclusion.\n",
        "    mcq_matches = re.findall(r'\\b([A-E])\\b', response_text)\n",
        "    if mcq_matches:\n",
        "        return mcq_matches[-1]\n",
        "\n",
        "    return None # Return None if no valid answer can be parsed\n",
        "\n",
        "def run_typhoon_agent(input_path: str, output_path: str):\n",
        "    df = pd.read_csv(input_path)\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
        "    model = AutoModelForCausalLM.from_pretrained(MODEL_ID, torch_dtype=torch.bfloat16, device_map=\"auto\")\n",
        "\n",
        "    stock_agent = QuantitativeStockAnalyzer()\n",
        "    compliance_agent = ComplianceAgentRAG()\n",
        "\n",
        "    predictions = []\n",
        "    for i, row in tqdm(df.iterrows(), total=len(df), desc=\"Typhoon Agent Processing\"):\n",
        "        query = row[\"query\"]\n",
        "        prediction = None # Start with a clean prediction\n",
        "\n",
        "        try:\n",
        "            query_type = classify_query(query)\n",
        "            prompt_data = {}\n",
        "\n",
        "            if query_type == \"stock_prediction\":\n",
        "                context_match = re.search(r'[Cc]ontext:(.*?)(?:\\n\\n\\d{4}|\\Z)', query, re.DOTALL)\n",
        "                tweets_match = re.search(r'\\d{4}-\\d{2}-\\d{2}:.*', query, re.DOTALL)\n",
        "                context = context_match.group(1).strip() if context_match else \"\"\n",
        "                tweets = tweets_match.group(0).strip() if tweets_match else \"\"\n",
        "                tech_summary, sentiment_summary = stock_agent.analyze(context, tweets)\n",
        "                prompt_data = get_surgical_prompt('stock_prediction', query=query, tech_analysis=tech_summary, sentiment_analysis=sentiment_summary)\n",
        "            elif query_type == \"ethical_scenario\":\n",
        "                relevant_principles = compliance_agent.retrieve_relevant_principles(query)\n",
        "                prompt_data = get_surgical_prompt('ethical_scenario', query=query, principles=relevant_principles)\n",
        "            else: # general_mcq\n",
        "                prompt_data = get_surgical_prompt('general_mcq', query=query)\n",
        "\n",
        "            messages = [{\"role\": \"system\", \"content\": prompt_data[\"system\"]}, {\"role\": \"user\", \"content\": prompt_data[\"user\"]}]\n",
        "\n",
        "            response = inference(messages, model, tokenizer)\n",
        "            prediction = extract_answer(response, query_type)\n",
        "\n",
        "            if prediction is None:\n",
        "                prediction = \"A\" # Fallback to default if parsing fails\n",
        "                print(f\"Warning: Could not extract answer for row {i+1}. Defaulting to 'A'. Raw: '{response[:100]}'\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"ERROR on row {i+1}: {e}\")\n",
        "            prediction = \"A\" # Fallback on critical error\n",
        "\n",
        "        predictions.append(prediction)\n",
        "\n",
        "    df[\"answer\"] = predictions\n",
        "    df[[\"id\", \"answer\"]].to_csv(output_path, index=False)\n",
        "    print(f\"\\n✅ Predictions saved to: {output_path}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    input_csv = '/content/combine_testsub.csv'\n",
        "    output_csv = \"submission.csv\"\n",
        "\n",
        "    run_typhoon_agent(input_csv, output_csv)"
      ],
      "metadata": {
        "id": "SI5CDlzTpZtR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}